{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IHLT_Utils.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOO59aoiqOFzHTx7nse+ra8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MateoProjects/UtilsAI/blob/main/IHLT_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftQproWqF9Qg"
      },
      "source": [
        "# **Utils for IHLT subject**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkmd6xRoGKO_"
      },
      "source": [
        "## **Import documents from drive**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivoe59eFF3PO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6b052d-5dc0-4f59-8dd1-c53d2e427177"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We6mZfViGZEr"
      },
      "source": [
        "**Note** : If you have a shared drive and you are not the owner of the folder or file you need to create a shortcut to your drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkrhpHcJGqN_"
      },
      "source": [
        "import pandas as pd\n",
        "dt = pd.read_csv('mypath',sep='\\t',header=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC7cs-2NHMWu"
      },
      "source": [
        "## **Imports and downloads**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzcr7LJtHQiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b08204-157b-4d9c-8db7-307f9382d9e1"
      },
      "source": [
        "!pip install python-crfsuite\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "from argparse import Namespace\n",
        "from google.colab import drive\n",
        "from scipy.stats import pearsonr # pearson correlation \n",
        "from nltk.metrics import jaccard_distance # calculate jaccard distance\n",
        "from nltk.stem import WordNetLemmatizer # for lemmas\n",
        "nltk.download('stopwords') # lab 1\n",
        "nltk.download('punkt') #lab 2 for tokenize words\n",
        "nltk.download('averaged_perceptron_tagger') # lab 3\n",
        "nltk.download('wordnet') # lab 3\n",
        "nltk.download('treebank') #lab 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1yacXWxCd-B"
      },
      "source": [
        "## **Standard functions for tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9FXJYeSCxzn"
      },
      "source": [
        "### 1. Sentence splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P40pAataCogF",
        "outputId": "541f004d-492f-408b-be2d-632375501101"
      },
      "source": [
        "source = 'Men want children. They get relaxed with kids.'\n",
        "sentences = nltk.sent_tokenize(source)\n",
        "sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Men want children.', 'They get relaxed with kids.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w4Xz3CFEGRO"
      },
      "source": [
        "### 2. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0nqmfv1ERRA",
        "outputId": "78a6dd42-e461-4f1f-ff17-4232ca94a3ba"
      },
      "source": [
        "s_list = [nltk.word_tokenize(s) for s in sentences]\n",
        "s_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Men', 'want', 'children', '.'],\n",
              " ['They', 'get', 'relaxed', 'with', 'kids', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRutmaW9EldE"
      },
      "source": [
        "**Note:** Depending on the needs, text can be splitted into sentences before tokenizing or it can be directly tokenized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjT25QYoFDb2"
      },
      "source": [
        "### 3. Exemple if we only tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgyaD1lZEsum",
        "outputId": "651ee979-b8c2-45e3-91b5-b00e2e6cc6ab"
      },
      "source": [
        "source = 'Men want children. They get relaxed with kids.'\n",
        "slist = [nltk.word_tokenize(s) for s in source]\n",
        "print(slist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['M'], ['e'], ['n'], [], ['w'], ['a'], ['n'], ['t'], [], ['c'], ['h'], ['i'], ['l'], ['d'], ['r'], ['e'], ['n'], ['.'], [], ['T'], ['h'], ['e'], ['y'], [], ['g'], ['e'], ['t'], [], ['r'], ['e'], ['l'], ['a'], ['x'], ['e'], ['d'], [], ['w'], ['i'], ['t'], ['h'], [], ['k'], ['i'], ['d'], ['s'], ['.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNoSUJD1_aY"
      },
      "source": [
        "## **Lemmatisation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF1mKYGr2KiF"
      },
      "source": [
        "wnl = WordNetLemmatizer() #create instance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gxHILsr2Yes"
      },
      "source": [
        "### 1. Functions for Lemmatisation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfKcf8Ki2dcx"
      },
      "source": [
        "# P is a tuple with (word, tag)\n",
        "\n",
        "def lemmatize(p):\n",
        "    if p[1][0] in {'N','V'}:\n",
        "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
        "    return p[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7RaM3t3HM2"
      },
      "source": [
        "* First we need to tokenize sentence\n",
        "* After tokenize sentence we need to identify and add tags of each word in sentence\n",
        "* Finally we can apply lemamatisation\n",
        "\n",
        "**Note**: The function below is a exemple of how can we program this steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkRhWGeI3H_l"
      },
      "source": [
        "# Sentence is a string\n",
        "\n",
        "def get_lemmas(sentence):\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  tags = nltk.pos_tag(words)\n",
        "  lemmas = [lemmatize(pair) for pair in tags]\n",
        "  return set(lemmas), tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GyXDyRaPgEY"
      },
      "source": [
        "## **Correlations**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFrxDcgpPzt4"
      },
      "source": [
        "### 1. The Pearson \n",
        "The Pearson correlation coefficient measures the linear relationship between two datasets.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "The function is: \n",
        "\n",
        "```\n",
        "pearsonr(x,y)\n",
        "```\n",
        "\n",
        "Where **x** and **y** are two datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brX5h5vLYCSs"
      },
      "source": [
        "from scipy.stats import pearsonr\n",
        "#assuming that dt exists and GS was another correlation that are added to dt. \n",
        "pearsonr(dt['gs'], dt['jaccard'])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNKb62xcjlvi"
      },
      "source": [
        "# **General Purpose**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBFoikObHjIS"
      },
      "source": [
        "## Functions or codes that can help us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgBsgRNVH1zn"
      },
      "source": [
        "### Sort dictionary by value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0G19tSGHwBN"
      },
      "source": [
        "# Assuming that we have a dictionari with {key: value} and value is a integer\n",
        "dict_ordered = sorted(mydict.items(), key=lambda x: x[1], reverse=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}